{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] KoNLPy 단어 토큰화 (1) Okt\n",
    "#### 토큰화 : 코퍼스에서 용도에 맞게 분류하는 작업\n",
    "- KoNLPy는 단어 토큰화가 아니라 정확히는 형태소단위로 형태소 토큰화(morpheme tokenization)를 수행  \n",
    "- 형태소란 의미를 가지는 최소의 단위  \n",
    "- 문구 (phrase)를 입력 받아 태깅된 형태소를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt  \n",
    "tagger = Okt() \n",
    "\n",
    "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "print(tagger.morphs(text))      # 형태소\n",
    "print(tagger.pos(text))         # 형태소 + 품사 태킹\n",
    "print(tagger.nouns(text))       # 명사 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] KoNLPy 단어 토큰화 (2) Hannanum\n",
    "#### Hannanum : 카이스트 SWRC 연구소에서 개발한 오픈소스 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "tagger = Hannanum()\n",
    "\n",
    "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "print(tagger.morphs(text))      # 형태소\n",
    "print(tagger.pos(text))         # 형태소 + 품사 태킹\n",
    "print(tagger.nouns(text))       # 명사 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] KoNLPy 단어 토큰화 (3) Kkma\n",
    "#### Okt 형태소 분석기와 다른 꼬꼬마(Kkma) 형태소 분석기로 단어 토큰화 실습\n",
    "- 앞서 실습한 Okt 형태소 분석기와는 다른 결과를 나타내는 것을 알 수 있음  \n",
    "- 형태소 분석기는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma  \n",
    "tagger = Kkma()  \n",
    "\n",
    "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "print(tagger.morphs(text))      # 형태소\n",
    "print(tagger.pos(text))             # 형태소 + 품사 태킹\n",
    "print(tagger.nouns(text))        # 명사 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] 정제 (cleaning)와 정규화 (normalization)\n",
    "#### 노이즈를 제거하고 같은 의미의 단어를 통합\n",
    "- 결측값과 이상치(outlier)를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "from collections import Counter\n",
    "\n",
    "tagger = Hannanum()\n",
    "\n",
    "text = \"대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 국민은 그 대표자나 국민투표에 의하여 주권을 행사한다.\"\n",
    "result = tagger.morphs(text)\n",
    "\n",
    "c = Counter(result)\n",
    "print(c.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"은\"과 \".\" 의미 없는 정보로 제거하고 분석해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"은\", \".\", \"이\", \"다\", \"의\", \"에게\", \"있\", \"고\", \"그\", \"에\", \"어\", \"을\", \"하\", \"ㄴ다\", \",\"]\n",
    "result = [x for x in result if x not in stop_words]\n",
    "\n",
    "c = Counter(result)## [강의] 정제 (cleaning)와 정규화 (normalization)\n",
    "#### 노이즈를 제거하고 같은 의미의 단어를 통합\n",
    ": 결측값과 이상치(outlier)를 제거\n",
    "print(c.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------- 주피터 노트북 커널을 리스타트 해 주세요 ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] 어근 동일화(stemming) - 한국어\n",
    "#### tagger.morphs(text, stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "tagger = Okt()\n",
    "text = \"한글 자연어 처리는 재밌다. 이제부터 열심히 해야지.\"\n",
    "\n",
    "print(tagger.morphs(text))\n",
    "print(tagger.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"한다 하겠다 할것이다\"\n",
    "print(tagger.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] KoNLPy 어간 추출\n",
    "#### 용언: ‘동사’, ‘형용사’는 어간과 어미의 결합 / KoNLPy를 활용한 용언 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "document = \"봄과 함께 찾아온 따뜻한 신제품 소식\"\n",
    "tagger = Okt()\n",
    "\n",
    "words = tagger.pos(document, stem=True) #stem=True 어근 형태\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 명사, 동사, 형용사만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = []\n",
    "for word in words:\n",
    "    if word[1] in [\"Noun\", \"Verb\", \"Adjective\"]:\n",
    "        clean_words.append(word[0])\n",
    "\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 명사만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = tagger.nouns(document) #명사 추출\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------- 주피터 노트북 커널을 리스타트 해 주세요 ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] KoNLPy 텍스트 전처리 후 워드 클라우드로 표현\n",
    "#### 뉴스 텍스트 데이터를 전처리해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "# 분석할 텍스트 읽어오기\n",
    "with open(\"news.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 불필요한 심볼 없애기\n",
    "p = re.compile(\"[\\Wa-zA-Z0-9_]+\")\n",
    "content = re.sub(p, \" \", content)\n",
    "\n",
    "# 형태소 분석 및 단어 추출\n",
    "tagger = Okt()\n",
    "morphs = tagger.pos(content)\n",
    "print(morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사만 추출하기\n",
    "words = []\n",
    "for word, pos in morphs:\n",
    "    if pos == \"Noun\":\n",
    "        words.append(word)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 불용어 제거\n",
    "stopwords = [\"출처\", \"뉴스\", \"원본\", \"링크\"]\n",
    "result = [x for x in words if x not in stopwords and len(x) > 1]\n",
    "print(result)\n",
    "\n",
    "# 빈도 분석, 각 단어들이 몇 번 사용되었는지 분석\n",
    "c = Counter(result)\n",
    "print(c.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 워드 클라우드로 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "\n",
    "# 한국어 경로 설정\n",
    "# 리눅스 : 나눔 고딕 폰트 설치 후 사용하기\n",
    "FONT_PATH = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "\n",
    "text = \" \".join(result)\n",
    "wordcloud = WordCloud(max_font_size=100, max_words=30, background_color=\"white\",\n",
    "                      relative_scaling=.5, font_path=FONT_PATH).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# 워드 클라우드 및 이미지를 파일로 저장\n",
    "wordcloud.to_file(\"news.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------- 주피터 노트북 커널을 리스타트 해 주세요 ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의 및 실습] 문재인 대통령 취임사 분석\n",
    "취임사 전문: https://news.joins.com/article/21558717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 로드 및 명사만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "\n",
    "f = open(\"문재인대통령취임사.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "text = \" \".join(lines)\n",
    "temp = []\n",
    "\n",
    "#명사만 추출한 후 추출된 명사를 리스트에 저장\n",
    "nouns = hannanum.nouns(text)\n",
    "for noun in nouns:\n",
    "    if len(noun) > 1 :\n",
    "        temp.append(noun)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 같은 의미를 지닌 단어를 하나로 합침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_words = {\n",
    "    \"국민들\" : \"국민\",\n",
    "    \"대통령의\" : \"대통령\"\n",
    "}\n",
    "temp_merged = []\n",
    "for word in temp:\n",
    "    for k, v in merge_words.items():\n",
    "        word = word.replace(k, v)\n",
    "    temp_merged.append(word)\n",
    "\n",
    "temp = temp_merged\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series 객체 생성 및 단어 빈도수 확인 (상위 10개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(temp)\n",
    "s1 = s.value_counts()\n",
    "s1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 같은 의미를 지닌 단어를 하나로 합침\n",
    "merge_words = {\n",
    "    \"국민들\" : \"국민\",\n",
    "    \"대통령의\" : \"대통령\"\n",
    "}\n",
    "temp_merged = []\n",
    "for word in temp:\n",
    "    for k, v in merge_words.items():\n",
    "        word = word.replace(k, v)\n",
    "    temp_merged.append(word)\n",
    "\n",
    "temp = temp_merged\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 워드 클라우드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 리눅스 : /usr/share/fonts\n",
    "wordcloud = WordCloud(\n",
    "    font_path=\"c:/Windows/Fonts/malgun.ttf\", \n",
    "    background_color=\"white\", \n",
    "    width=800, \n",
    "    height=800)\n",
    "wordcloud = wordcloud.generate(\" \".join(temp))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "fig.savefig(\"wordcloud.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
