{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장에서 추출한 단어들에 인덱스 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "phrase = \"나는 자연어 처리를 배운다\"\n",
    "okt = Okt()\n",
    "token = okt.morphs(phrase)\n",
    "print(token)\n",
    "\n",
    "word_set = {}\n",
    "index = 0\n",
    "for t in token:\n",
    "    if t not in word_set:\n",
    "        word_set[t] = index\n",
    "        index += 1\n",
    "\n",
    "print(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 특정 단어를 One-hot encoding 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word_set):\n",
    "    vector = [0] * len(word_set)\n",
    "    index = word_set[word]\n",
    "    vector[index] = 1\n",
    "    return vector\n",
    "\n",
    "print(word_set)\n",
    "print(one_hot_encoding(\"자연어\", word_set))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] Bag of Words (BoW)\n",
    "#### BoW는 단어들의 순서는 전혀 고려하지 않고 출현 빈도에 집중한 텍스트 데이터의 수치 표현 방법\n",
    "#### BoW를 만드는 과정\n",
    "- 각 단어에 고유한 정수 인덱스를 부여한다.  \n",
    "- 각 인덱스의 위치에 단어 토근의 등장 횟수를 기록한 벡터를 생성한다.  \n",
    "- Ont-hot은 있냐 없냐 만 나타내는 방식이었음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 단어별 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "# 전처리\n",
    "text1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "text1 = re.sub(r\"\\.\", \"\", text1)\n",
    "\n",
    "# 형태소 분리\n",
    "okt = Okt()\n",
    "token = okt.morphs(text1)\n",
    "\n",
    "# 단어별 인덱싱\n",
    "bow = {}\n",
    "index = 0\n",
    "for t in token:\n",
    "    if t not in bow:\n",
    "        bow[t] = index\n",
    "        index += 1\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 단어별 빈도를 구한 후 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화\n",
    "vec = []\n",
    "for w, i in bow.items():\n",
    "    count = token.count(w)\n",
    "    vec.append(count)\n",
    "\n",
    "print(vec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의] 사이킷런을 이용한 특징 추출(벡터화)\n",
    "#### Scikit-learn은 분류, 회귀, 군집화, 의사결정 트리 등 머신러닝 알고리즘 함수를 제공하는 라이브러리\n",
    "#### 텍스트 분석과 관련한 라이브러리도 사용가능\n",
    "- CountVectorizer : 텍스트에서 횟수를 기준으로 특징을 추출하는 방법 (Bag of Words와 동일)  \n",
    "- TFIDFVectorizer : TF-IDF라는 값(단어의 가중치)을 사용해서 텍스트에서 특징을 추출  \n",
    "- HashingVectorizer : 해시 함수를 사용하여 적은 메모리와 빠른 속도를 통해 실행 시간이 단축 됨  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CountVectorizer : 텍스트에서 횟수를 기준으로 특징을 추출하는 방법 (Bag of Words와 동일)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터로부터 단어 사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = [\n",
    "    \"나는 배가 고프다\",\n",
    "    \"내일 점심 뭐먹지\",\n",
    "    \"내일 공부 해야겠다.\",\n",
    "    \"점심 먹고 공부해야지\"\n",
    "]\n",
    "\n",
    "# 단어 사전 구성\n",
    "count_vectorizer = CountVectorizer()     # 객체 생성\n",
    "count_vectorizer.fit(text_data)          # 단어 목록 생성\n",
    "print(count_vectorizer.vocabulary_)\n",
    "print(len(count_vectorizer.vocabulary_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 사전을 이용해서 문장을 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 코드 생략\n",
    "# 문장을 벡터화\n",
    "sentence = [text_data[0]]       # 첫 번째 문장\n",
    "vector = count_vectorizer.transform(sentence).toarray()          # transform\n",
    "print(vector)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TfidfVectorizer : TF-IDF라는 특정 값을 사용해서 텍스트 데이터의 특징을 추출하는 방법\n",
    "- TF(Term Frequency): 특정 문서에서 특정 단어가 등장한 횟수\n",
    "- DF(Document Frequency): 특정 단어가 등장한 문서의 수\n",
    "- IDF(Inverse Document Frequency): DF의 역수\n",
    "- DF-IDF는 DF * IDF를 의미함\n",
    "- 어떤 단어가 해당 문서에는 자주 등장하지만 다른 문서에는 많이 없는 단어일 수록 높은 값을 가지게 됨\n",
    "- 조사나 지시대명사처럼 자주 등장하는 단어는 TF 가 크지만 IDF가 작아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = [\n",
    "    \"나는 배가 고프다\",\n",
    "    \"내일 점심 뭐먹지\",\n",
    "    \"내일 공부 해야겠다.\",\n",
    "    \"점심 먹고 공부해야지\"\n",
    "]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(text_data)\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "sentence = [text_data[3]]\n",
    "vector = tfidf_vectorizer.transform(sentence).toarray()\n",
    "print(vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [강의 및 실습] 유사도를 이용한 영화 추천 시스템\n",
    "### TF-IDF와 코사인 유사도만으로 영화의 줄거리에 기반해서 영화를 추천하는 시스템을 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    # TF-IDF 벡터화\n",
    "from sklearn.metrics.pairwise import linear_kernel      # 코사인 유사도 계산\n",
    "\n",
    "# 데이터 읽은 후 20000개 샘플만 읽기\n",
    "data = pd.read_csv(\"movies_metadata.csv\", low_memory=False)\n",
    "data = data.iloc[:20000]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) overview가 없는 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"overview\"] = data[\"overview\"].fillna(\"\")\n",
    "print(data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) TF-IDF matrix 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\") \n",
    "tfidf_matrix = tfidf.fit_transform(data[\"overview\"])\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 코사인 유사도 계산하기 (similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 특정 영화 제목이 주어졌을 때 이 영화와 유사도가 가장 높은 10개 영화 제목을 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "#print(cosine_sim[0][:10])\n",
    "\n",
    "title = \"The Dark Knight Rises\"\n",
    "title2index = pd.Series(data=data.index, index=data[\"title\"]).drop_duplicates()\n",
    "index = title2index[title]\n",
    "\n",
    "# index 번째 영화에 대한 유사도 값에 대해 [(0, 유사도), (1, 유사도) ..] 형태로 저장\n",
    "sim_scores = list(enumerate(cosine_sim[index]))\n",
    "\n",
    "# 정렬하기\n",
    "sim_scores.sort(key=lambda x:x[1], reverse=True)\n",
    "top10 = sim_scores[1:11]\n",
    "top10_index = [x[0] for x in top10]\n",
    "print(top10_index)\n",
    "print(data[\"title\"].iloc[top10_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
